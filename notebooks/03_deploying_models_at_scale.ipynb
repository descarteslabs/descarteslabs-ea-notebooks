{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e2a444c",
   "metadata": {},
   "source": [
    "# Deploying a model at scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16e8986",
   "metadata": {},
   "source": [
    "In this notebook we will be taking a pre-trained model and deploying it using the DL Tasks and Scenes APIs. The model takes an interpolated cloud-free aggregated timeseries of Sentinel-2 L2A NDVI and predicts whether or not a given field is corn. The model was trained using CDL data for 2019 and should be used only as a reasonable stand-in for the types of models you may be interested in deploying on the DL platform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce32573",
   "metadata": {},
   "source": [
    "We will use the following DL API's in this exercise:\n",
    "- [Scenes](https://docs.descarteslabs.com/descarteslabs/scenes/readme.html) - Query for and access imagery over our AOI\n",
    "- [Storage](https://docs.descarteslabs.com/descarteslabs/client/services/storage/readme.html) - Store our model on the DL backend/cloud data store\n",
    "- [Tasks](https://docs.descarteslabs.com/descarteslabs/client/services/tasks/readme.html) - Deploy data pipeline and model code on dsitributed DL backend\n",
    "\n",
    "We will use the following external Python packages:\n",
    "- [geopandas](https://geopandas.org/en/stable/docs.html) - Import, transform, and query our reference dataset for Iowa agricultural fields\n",
    "- [scipy.interpolate](https://docs.scipy.org/doc/scipy/reference/interpolate.html) - Interpolate our NDVI tseries onto a regular temporal grid\n",
    "- [matplotlib.pyplot](https://matplotlib.org/3.5.0/api/_as_gen/matplotlib.pyplot.html) - Plot imagery\n",
    "- [numpy](https://numpy.org/doc/stable/index.html) - Array/imagery operations and manipulations\n",
    "- [datetime](https://docs.python.org/3/library/datetime.html) - Create date ranges and timestamps for interpolation\n",
    "- [joblib](https://joblib.readthedocs.io/en/latest/) - Save and load model\n",
    "- [tqdm](https://tqdm.github.io/) - Fancy progress bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74097754",
   "metadata": {},
   "outputs": [],
   "source": [
    "import descarteslabs as dl\n",
    "from descarteslabs.client.services.tasks import as_completed\n",
    "import geopandas as gpd\n",
    "from scipy.interpolate import interp1d\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from joblib import dump, load\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1d0b85",
   "metadata": {},
   "source": [
    "We start by writing a function that combines a series of data access steps (as in the previous notebook) and data manipulations/refinements. We use `dl.scenes.search` to find imagery over an input geometry and date range. Then we pull the red, nir, and cloud mask bands from that imagery into a single stack aranged by day. Similar to the last notebook we then mask out any cloudy pixels and pixels outside the input geometry. This results in an array of shape *(time/days, xs, ys)*. NDVI is then spatially aggregated over the input geometry leaving a single time series of shape *(time/days, NDVI value)*. This time series need to be interpolated onto a standard temporal grid so that it can match the input data for the model. This interpolation is done by converting the dates of each image into a timestamp and interpolating onto a new grid of timestamp values between the input start and end dates. The interpolation is then done by `scipy.interpolate.interp1d`. The interpolated time series and corresponding dates are then returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28ef766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ndvi_tseries(\n",
    "    geom, \n",
    "    start_date=\"2019-04-01\", \n",
    "    end_date=\"2019-10-01\"\n",
    "):\n",
    "    scenes, ctx = dl.scenes.search(\n",
    "        geom,\n",
    "        products=\"esa:sentinel-2:l2a:v1\",\n",
    "        start_datetime=start_date,\n",
    "        end_datetime=end_date,\n",
    "        limit=None\n",
    "    )\n",
    "    print(f\"Found {len(scenes)} scenes for specified geometry\")\n",
    "    \n",
    "    print(f\"Pulling raster data from DL Catalog\")\n",
    "    stack = scenes.stack(\n",
    "        [\"red\", \"nir\", \"cloud_mask\"],\n",
    "        ctx,\n",
    "        flatten=lambda x: x.properties.date.strftime(\"%Y-%m-%d\"),\n",
    "        scaling=\"physical\"\n",
    "    )\n",
    "    \n",
    "    print(f\"Masking out clouds\")\n",
    "    cmask = np.repeat(\n",
    "        (stack[:,-1].data==1)[:, np.newaxis],\n",
    "        stack.shape[1],\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    stack.mask = (stack.mask) | cmask\n",
    "    \n",
    "    print(f\"Computing NDVI\")\n",
    "    ndvi = (stack[:,1] - stack[:,0])/(stack[:,1] + stack[:,0])\n",
    "\n",
    "    ndvi_ts = np.ma.median(ndvi, axis=[1,2])\n",
    "    dates = list(scenes.groupby(\"properties.date.day\"))\n",
    "    \n",
    "    dates = [\n",
    "        key for key, scene in scenes.groupby(\n",
    "            lambda x: x.properties.date.strftime(\"%Y-%m-%d\")\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    dates_ts = [\n",
    "        datetime.strptime(date, \"%Y-%m-%d\").timestamp() for date in dates\n",
    "    ]\n",
    "    \n",
    "    new_dates = np.arange(\n",
    "        datetime.strptime(start_date, \"%Y-%m-%d\"),\n",
    "        datetime.strptime(end_date, \"%Y-%m-%d\"),\n",
    "        timedelta(days=6)\n",
    "    ).astype(datetime)\n",
    "    \n",
    "    new_dates_ts = [t.timestamp() for t in new_dates]\n",
    "    \n",
    "    tseries_masked = ndvi_ts.data[~ndvi_ts.mask]\n",
    "    dates_masked = np.array(dates_ts)[~ndvi_ts.mask]\n",
    "    \n",
    "    print(f\"Interpolating time series from dates: {dates} to new dates: {new_dates.tolist()}\")\n",
    "    \n",
    "    f_interp = interp1d(\n",
    "        dates_masked,\n",
    "        tseries_masked,\n",
    "        bounds_error=False,\n",
    "        copy=False,\n",
    "        fill_value=\"extrapolate\",\n",
    "    )\n",
    "    \n",
    "    return f_interp(new_dates_ts)[1:], new_dates[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a9b381",
   "metadata": {},
   "source": [
    "## Loading reference data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0915facc",
   "metadata": {},
   "source": [
    "We load in the field geometries in the same way as the previous notebook. These are the geometries we'll be feeding to our `get_ndvi_tseries` function above. We need to convert the reference fields to EPSG 4326."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce99ec99",
   "metadata": {},
   "outputs": [],
   "source": [
    "ia_fields = gpd.read_file(\"../data/IowaFieldBoundaries2019.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24652793",
   "metadata": {},
   "outputs": [],
   "source": [
    "ia_fields = ia_fields.to_crs(\"EPSG:4326\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7236f7",
   "metadata": {},
   "source": [
    "## Testing our `get_ndvi_tseries` function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa38db5",
   "metadata": {},
   "source": [
    "Let's grab a test geometry from our fields dataset. We can test out function using this test geometry to examine what exactly this function returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24413834",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_geom = sg.mapping(ia_fields.iloc[2000].geometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0371524",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndvi_ts, ndvi_dates = get_ndvi_tseries(test_geom)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02d0b6b",
   "metadata": {},
   "source": [
    "We plot the returned NDVI time series and dates. We can see a fairly reasonable NDVi curve (with higher NDVI in the growing season)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22044f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ndvi_dates, ndvi_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8eae10",
   "metadata": {},
   "source": [
    "## Creating a `CloudFunction`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e6c8a2",
   "metadata": {},
   "source": [
    "Now that we have our function for pulling a clean interpolated timeseries we need to write a function that takes that function and gets the timeseries, loads our model, then returns a prediction. We start by writing our model to DL Storage. We do this so that the model can be loaded from the cloud into each task being run on the DL backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327b030d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl.storage.set_file(\"classifier.joblib\", \"../models/classifier.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c485324",
   "metadata": {},
   "source": [
    "Now let's look at the function below: `classify_ndvi`. We've taken the `get_ndvi_tseries` function and put it into a `utils.py` file. This function imports the `get_ndvi_tseries` function, `joblib.load` and the dl client. We generate a clean timeseries, retrieve the model from storage, load the model, then return a prediction. The function takes both a geometry and a unique id as input. The `field_id` argument isn't necessary but will simplify writing the results of our model back into the reference `GeoDataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c2bde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_ndvi(geom, field_id):\n",
    "    from utils import get_ndvi_tseries\n",
    "    from joblib import load\n",
    "    import descarteslabs as dl\n",
    "    \n",
    "    ndvi_ts, ndvi_dates = get_ndvi_tseries(geom)\n",
    "    \n",
    "    dl.storage.get_file(\"classifier.joblib\", \"classifier.joblib\")\n",
    "    clf = load(\"classifier.joblib\")\n",
    "    \n",
    "    return clf.predict(ndvi_ts.reshape(1,-1)), field_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2c1a50",
   "metadata": {},
   "source": [
    "We test our `classify_ndvi` function below using a test geometry and print our the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e75c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict, test_fid = classify_ndvi(test_geom, ia_fields.iloc[2000].FBndID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d135a083",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b22a22",
   "metadata": {},
   "source": [
    "We take the `classify_ndvi` function and now \"turn it into\" a `CloudFunction`. We do this by using `dl.tasks.create_function`. This will wrap our function and send it to the DL backend to be deployed. The function can be viewed in the task monitor UI [here](https://monitor.descarteslabs.com/). This UI displays your asctive task groups. A task group is the resources/build that will run your tasks as you submit them with your `CloudFunction`.\n",
    "\n",
    "When we make our `CloudFunction` we also specify the name, [DL provided Docker image](https://docs.descarteslabs.com/guides/tasks.html#choosing-your-environment), how many concurrent workers we would like, modules to include, Python package requirements, number of CPUs, and memory amount. There are other parameters you can specify for your tasks function as well. Please consult the docs [here](https://docs.descarteslabs.com/descarteslabs/client/services/tasks/readme.html#descarteslabs.client.services.tasks.Tasks.create_function) for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469cd5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "async_func = dl.tasks.create_function(\n",
    "    classify_ndvi,\n",
    "    name=\"NDVI classifier prediction\",\n",
    "    image=\"us.gcr.io/dl-ci-cd/images/tasks/public/py3.7:v2022.01.20-7-gc73f23f4\",\n",
    "    maximum_concurrency=150,\n",
    "    include_modules=[\"utils\"],\n",
    "    requirements=[],\n",
    "    cpus=1,\n",
    "    memory=\"2Gi\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b8f123",
   "metadata": {},
   "source": [
    "We can submit a single job in the same way we use the `classify_ndvi` function. We simply supply the arguments to the returned `CloudFunction` object as you would the original function. We do this below for our test geometry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9269a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "async_func(test_geom, ia_fields.iloc[2000].FBndID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6548c31d",
   "metadata": {},
   "source": [
    "## Scaling up our deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dcc227",
   "metadata": {},
   "source": [
    "Now that we have our `CloudFunction` let's submit a number of jobs to our task group. We start by taking a random subset of our reference fields dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd9bbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_idx = np.random.choice(range(len(ia_fields)), size=500)\n",
    "ia_fields_predict_sample = ia_fields.iloc[predict_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60284833",
   "metadata": {},
   "source": [
    "Using that sample we now create a list of geometries we'd like to submit. We also create a list of the unique field ids found in the `FBndID` column of our reference data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06144f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "geoms_predict = list(map(lambda k: sg.mapping(k.buffer(0)), ia_fields_predict_sample.geometry))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aac3078",
   "metadata": {},
   "outputs": [],
   "source": [
    "field_ids_predict = list(ia_fields_predict_sample.FBndID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d624ca5",
   "metadata": {},
   "source": [
    "With these two lists we can the use the `.map()` method of our `CloudFunction` and submit all 500 jobs quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dfb5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_tasks = async_func.map(geoms_predict, field_ids_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25f03b0",
   "metadata": {},
   "source": [
    "You should now see a number of jobs in the task monitor UI under the task group you submitted earlier. For more information on the task monitor and what it shows you about your running tasks please see this article [here](https://docs.descarteslabs.com/ui/monitor.html?highlight=monitor)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d693e41d",
   "metadata": {},
   "source": [
    "We then get our sample reference dataset ready to right the results of the model deployment back into our original array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8644db",
   "metadata": {},
   "outputs": [],
   "source": [
    "ia_fields_predict_sample = ia_fields_predict_sample.set_index(\"FBndID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054ab3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ia_fields_predict_sample[\"class\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335d3974",
   "metadata": {},
   "source": [
    "Finally we wait for the tasks to complete. We can use the `as_completed` function from the Tasks api to check for tasks as they complete. As each task completes we write the result into our sample reference dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44f3562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the shape of the image array returned by each task\n",
    "print(\"starting to wait for task completions\")\n",
    "failed_tasks = []\n",
    "for task in tqdm(as_completed(predict_tasks, show_progress=False), total=len(predict_tasks)):\n",
    "    if task.is_success:\n",
    "        cid, field_id = task.result\n",
    "        ia_fields_predict_sample.loc[field_id, \"class\"] = cid\n",
    "    else:\n",
    "        failed_tasks.append(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d5df08",
   "metadata": {},
   "source": [
    "Should the need arise we can resubmit any jobs that fail in a tasks group by using the `dl.tasks.rerun_failed_tasks()` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a7877c",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_id = async_func.group_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214748b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl.tasks.rerun_failed_tasks(group_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb569dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ia_fields_predict_sample"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
