{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b580a358-8b8c-4c09-8592-0bd31c0e7e4d",
   "metadata": {},
   "source": [
    "# Running models at scale using DL & AWS Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fffaf178-2ac2-4d2e-9b86-7e6219f8b14c",
   "metadata": {},
   "source": [
    "In the previous notebook we ran our model using AWS Lambda. Although we can use Lambda to asynchronously execute our model we will often run into scenarios where we need more resources for a job or need more control over the execution of a job. In these scenarios we can leverage another AWS service, Batch. Batch provides a scalable compute environment on which we can execute our model. In this notebook we will be running the same model from our previous notebook but using Batch instead of Lambda."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a63b67d-6952-43f2-aeaf-f2e0e1f2b52d",
   "metadata": {},
   "source": [
    "## Runtime code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddcdf48-2f90-40f0-9269-1842506194e4",
   "metadata": {},
   "source": [
    "For our Batch deployment we will need to slightly modify our runtime code from our Lambda example. We will be using `click` to simplify the command we pass along when submitting jobs. The runtime code can be found in `../app/dl_aws_batch.py` or seen below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada15d77-ea0d-4778-af84-9e53cffddc02",
   "metadata": {},
   "source": [
    "```python\n",
    "from .model import get_field_class\n",
    "\n",
    "import click\n",
    "import logging\n",
    "import json\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "@click.command()\n",
    "@click.argument(\n",
    "    \"geom_str\",\n",
    "    type=click.STRING\n",
    ")\n",
    "@click.argument(\n",
    "    \"fid\",\n",
    "    type=click.STRING\n",
    ")\n",
    "@click.argument(\n",
    "    \"s3_bucket\",\n",
    "    type=click.STRING\n",
    ")\n",
    "@click.argument(\n",
    "    \"model_name\",\n",
    "    type=click.STRING\n",
    ")\n",
    "def run_batch_field_model(\n",
    "    geom_str,\n",
    "    fid,\n",
    "    s3_bucket,\n",
    "    model_name\n",
    "):\n",
    "    geom = json.loads(geom_str)\n",
    "    result = get_field_class(geom, fid, s3_bucket, model_name)\n",
    "    logger.info(result)\n",
    "\n",
    "    return result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_batch_field_model()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fbaf20-bf8d-44ca-97ae-58c7c9f4bbab",
   "metadata": {},
   "source": [
    "Unlike in our Lambda example we will be passing in the S3 bucket and model name when we submit the jobs so these are added as arguments for our `run_batch_field_model()` function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567afcaa-c784-400e-af71-f44b1f88b277",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Building a Docker image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b9af90-cc46-475c-b71c-6c6a9a3e8ba4",
   "metadata": {},
   "source": [
    "As in the previous notebook we will need to build a special Docker image to be run in Batch. You can find the Dockerfile for this notebook in `../dockerfiles/batch/Dockerfile`. The file is fairly similar to the one we used in the Lambda example but with a few subtle differences:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca933d8b-2673-4fd2-9fdc-f04bb8636a7f",
   "metadata": {},
   "source": [
    "```Dockerfile\n",
    "FROM python:3.9-slim-buster\n",
    "\n",
    "COPY app app\n",
    "\n",
    "COPY requirements.txt requirements.txt\n",
    "RUN pip3 install -r requirements.txt\n",
    "RUN pip3 install -U descarteslabs>=1.11.0 \n",
    "\n",
    "ENV DESCARTESLABS_ENV=aws-production\n",
    "\n",
    "RUN mkdir /tmp/models\n",
    "\n",
    "ENTRYPOINT [\"python3\", \"-m\", \"app.dl_aws_batch\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4004426c-af83-4af6-9de6-8f470daeca96",
   "metadata": {},
   "source": [
    "We use a different base image here `python:3.9-slim-buster` and do not copy the application code to a Lambda specific execution directory. We also specify an `ENTRYPOINT` rather than a `CMD`. Both are ways to specify what code to run in the container but for this example an `ENTRYPOINT` will simplify things slightly when we submit out job."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b66127f-c8c5-4872-9227-c47e2c6d7769",
   "metadata": {},
   "source": [
    "We now need to build our image and push it to the ECR. For more information on these steps please see the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f51a4e-e308-4d1e-8d6c-464c8f64d569",
   "metadata": {},
   "source": [
    "- [Creating an ECR repository](https://docs.aws.amazon.com/AmazonECR/latest/userguide/repository-create.html)\n",
    "- [Build and push your Docker image](https://docs.aws.amazon.com/lambda/latest/dg/images-create.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0841ecd2-b513-4d3d-83c4-bb6c2c75ec56",
   "metadata": {},
   "source": [
    "The general steps for this process look something like this though:\n",
    "1) `cd ~/dl-ea-aws-onboarding`\n",
    "2) `docker build -t dl-aws-onboarding-batch -f dockerfiles/batch/Dockerfile .` You can specify a different name for your image by swapping our \"dl-aws-onboarding-batch\" for something else\n",
    "3) `docker tag dl-aws-onboarding-batch:latest {your-container-registry}/dl-aws-onboarding-batch:latest`\n",
    "4) `docker push {your-container-registry}/dl-aws-onboarding-batch:latest`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea113402-5a69-4b22-931b-7a5c2a1f83bf",
   "metadata": {},
   "source": [
    "`docker build -t dl-aws-onboarding-batch -f dockerfiles/batch/Dockerfile .`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686f7420-1c37-4808-9a03-5b2f0b36018a",
   "metadata": {},
   "source": [
    "`docker tag dl-aws-onboarding-batch:latest 851517463584.dkr.ecr.us-west-2.amazonaws.com/dl-aws-onboarding-batch:latest`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439482e4-bd13-467d-99f3-56f9e9996e67",
   "metadata": {},
   "source": [
    "`docker push 851517463584.dkr.ecr.us-west-2.amazonaws.com/dl-aws-onboarding-batch:latest`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b336d199-93eb-4f86-8525-379f62043b02",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Creating IAM role for Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d520cc9-d0db-4f02-a4c8-ad37a747835f",
   "metadata": {},
   "source": [
    "We will now need to create some infrastructure pieces to support our Batch deployment. We need to first create an IAM role (permissions) to be used for executing out project code. This IAM role will need access to a few different services to be able to properly run the model. Please note that this notebook only provides a rough outline of how to properly create an IAM role for your application. When adjust permissions and creating roles and policies please proceed with caution to avoid any issues with security. Descartes Labs, Inc. is not responsible for any issues arising from improperly specified AWS security infrastructure. Please see the following docs for more information on how to tailor your role and policies to your needs: https://docs.aws.amazon.com/iam/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e67e68-f43c-4949-ab3b-9d3e3cdd813a",
   "metadata": {},
   "source": [
    "We will be specifying our infrastructure on AWS using `boto3` (AWS's python API)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975cd7ef-7cb1-4e27-9e5f-9976816253fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import geopandas as gpd\n",
    "import shapely.geometry as sg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c682ddcd-45a1-4981-b99e-0e40e7285ed7",
   "metadata": {},
   "source": [
    "We start be instantiating the iam client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a988d287-4207-4cf9-884a-df861b4815af",
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_iam = boto3.client('iam')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef74366a-81f4-4bac-bc2a-a65f90e6988c",
   "metadata": {},
   "source": [
    "We then need to allow our IAM role to assume two roles to be able to use relevant services: S3 and ECS Tasks. These two roles will allow us to access and use S3 buckets and to use Batch/ECS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c318c782-cf7b-4933-9558-a225dfea91b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "assume_role_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Action\": \"sts:AssumeRole\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"s3.amazonaws.com\"\n",
    "            },\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Sid\": \"\"\n",
    "        },\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"ecs-tasks.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7cff9c-4b48-40da-b664-24d0bb85f0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "role_response = aws_iam.create_role(\n",
    "    RoleName='dl-aws-onboarding-batch-role',\n",
    "    AssumeRolePolicyDocument=json.dumps(assume_role_policy)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bc3c99-b035-4a65-8b25-55bd98fb28ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "role_response[\"Role\"][\"Arn\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d8da36-e855-466e-a691-0359870d200d",
   "metadata": {},
   "source": [
    "### Attaching policies to the execution role"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace123cc-174a-4c9b-9fe7-059972f7068d",
   "metadata": {},
   "source": [
    "We now need to attach policies to this role to allow it to use specific AWS services. The policies we care about for this example are: S3 Access, EC2 Container Registry, Cloud Watch Logs, and Secrets Manager. For more information about policies please see [these docs](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies.html). Again you may want to specify more specific and limited policies for your IAM role. For more information about this please see [here](https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_create.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aed9ee9-6288-4f4a-9f8f-895946235175",
   "metadata": {},
   "outputs": [],
   "source": [
    "execution_role = aws_iam.get_role(RoleName='dl-aws-onboarding-batch-role')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a6e67b-2237-4929-a35b-194916b7c5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_iam.attach_role_policy(\n",
    "    RoleName=execution_role[\"Role\"][\"RoleName\"],\n",
    "    PolicyArn=\"arn:aws:iam::aws:policy/AmazonS3FullAccess\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918ede2e-5baf-4022-b53b-30bf4de8d7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_iam.attach_role_policy(\n",
    "    RoleName=execution_role[\"Role\"][\"RoleName\"],\n",
    "    PolicyArn=\"arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd56dbcb-6493-4597-b189-efd8612ca3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_iam.attach_role_policy(\n",
    "    RoleName=execution_role[\"Role\"][\"RoleName\"],\n",
    "    PolicyArn=\"arn:aws:iam::aws:policy/CloudWatchLogsFullAccess\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946f06fd-5ae7-4d60-8afe-9376a6a39c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_iam.attach_role_policy(\n",
    "    RoleName=execution_role[\"Role\"][\"RoleName\"],\n",
    "    PolicyArn=\"arn:aws:iam::aws:policy/SecretsManagerReadWrite\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ce17f2-8edd-41e6-87c1-ff8ea2491c85",
   "metadata": {},
   "source": [
    "## Create a compute environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bdb3c6-c02d-45e4-96d4-f4961dda8b33",
   "metadata": {},
   "source": [
    "Although you can create a compute environment using boto3 we will instead use the management console. Navigate to Batch and \"Compute environments\". Then select \"Create\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760c69cf-ef78-4980-8ca3-e92027000eb7",
   "metadata": {},
   "source": [
    "<img src=\"../images/create_compute_env_batch.png\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257a44f7-2ed6-43f5-b3db-34bdda8f27a2",
   "metadata": {},
   "source": [
    "Now provide a name for your compute environment. Keep everything default until you select the \"Instance Configuration\". For this example choose either \"Fargate\" or \"Fargate Spot\". Please note that the cost of these services will differ depending on your selection. Please consult [the pricing documentation](https://aws.amazon.com/batch/pricing/) for more info on this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da06f0dc-cda8-4d8b-b5dc-4ab23fedc2a5",
   "metadata": {},
   "source": [
    "<img src=\"../images/instance_config.png\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36464661-e31d-4299-802d-e00a4c6e2bd4",
   "metadata": {},
   "source": [
    "You will also need to specify a maximum number of vCPUs that can be leveraged by your compute environment. For this example you can specify something lower (32 for example)/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcdb204-dd27-4abe-a0d2-16d3ae74e088",
   "metadata": {},
   "source": [
    "Under the networking section you will likely want to define a specific VPC to use for this compute environment. For more information on this please consult the [AWS docs here](https://docs.aws.amazon.com/batch/latest/userguide/get-set-up-for-aws-batch.html#create-a-vpc)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6617748a-eef4-4dd4-9295-9d70042e5530",
   "metadata": {},
   "source": [
    "Finally you can click \"Create compute environment\"!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6437abfa-9e36-4b4e-bf5a-7c0ca6cac9d2",
   "metadata": {},
   "source": [
    "## Creating job queue and job definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d5bc21-18a6-45fa-94b2-a035a708b9f7",
   "metadata": {},
   "source": [
    "Now that we have a compute environment we need to create [a job queue](https://docs.aws.amazon.com/batch/latest/userguide/job_queues.html) and [a job definition](https://docs.aws.amazon.com/batch/latest/userguide/job_definitions.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52112561-d75b-42bb-9028-7d00c4e2f44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = boto3.client(\"batch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affd3ecb-a146-435f-aa89-6d07050cd389",
   "metadata": {},
   "source": [
    "We need to specify a few variables to use in our `boto3` calls for creating our queue and definition. The values in the cells below need to be specified to match your compute environment and Docker image in the AWS ECR. The other values are used to specify the resources, retries, and timeouts associated with the jobs we will be running. For more info on these please see [the docs here](https://docs.aws.amazon.com/batch/latest/userguide/job_definition_parameters.html). There are also [specific resource limitations](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-cpu-memory-error.html) between numbers of cpus and memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f11ca9-a885-4329-905b-ce2b2b55150f",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_env = \"dl-aws-onboarding-batch\"\n",
    "image = \"851517463584.dkr.ecr.us-west-2.amazonaws.com/dl-aws-onboarding-batch:latest\"\n",
    "\n",
    "timeout = 600\n",
    "vcpu = 1\n",
    "memory = 2048\n",
    "retries = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b0e828-1500-47a9-8c51-3feaf5d84d35",
   "metadata": {},
   "source": [
    "### Job queue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d69428-510e-4690-aacb-d87615116002",
   "metadata": {},
   "source": [
    "We start by defining the queue. We must provide a name, the compute environment, and a priority."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a73410-0d76-4c58-af8e-51ac49e17f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the \"name\" of the job queue and definition\n",
    "queue_name = \"dl-aws-onboarding-batch-queue\"\n",
    "\n",
    "# Create the job queue\n",
    "response = batch.create_job_queue(\n",
    "    jobQueueName=queue_name,\n",
    "    state=\"ENABLED\",\n",
    "    priority=1,\n",
    "    computeEnvironmentOrder=[\n",
    "        {\n",
    "            \"order\": 0,\n",
    "            \"computeEnvironment\": compute_env,\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "job_queue_arn = response[\"jobQueueArn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66155fc6-8b73-4602-bf69-d512b2570483",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(job_queue_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c6cabb-97af-447e-b4d6-fd3d350687dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_queue_arn = \"arn:aws:batch:us-west-2:851517463584:job-queue/dl-aws-onboarding-batch-queue\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdde7354-b861-4a99-8545-5584cd45d5b5",
   "metadata": {},
   "source": [
    "### Creating a DL Auth secret in AWS Secret Manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de4d645-124e-4e2a-bb7a-c9f0d3dda36f",
   "metadata": {},
   "source": [
    "Before we create our job definition we will want to add our DL credentials to the AWS Secret Manager so that we can authenticate to use the DL services from our Batch jobs. For more information on how to do this [please see the docs](https://docs.aws.amazon.com/secretsmanager/latest/userguide/create_secret.html). You will need to store your client id and secret in the manager and then get the ARN associated with the secret to use in your job definition. The previous notebook on Lambda has details on how to get your client id and secret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0b4dd2-4e45-49bb-b8ea-764702ea342c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_auth_secret_name = \"arn:aws:secretsmanager:us-west-2:851517463584:secret:dylan_aws_dlauth_creds-dfmj5U\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6747d1-a2a5-45eb-b2bb-ee4ba832ba9f",
   "metadata": {},
   "source": [
    "### Job definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1145f7-bd6d-40c0-b43b-bb6ac017796b",
   "metadata": {},
   "source": [
    "We can now create our job definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83035011-b650-4500-9dde-5bac39df88d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_def_name = \"dl-aws-onboarding-batch-definition\"\n",
    "# If the job definition doesn't exist, create it\n",
    "response = batch.register_job_definition(\n",
    "    jobDefinitionName=job_def_name,\n",
    "    type=\"container\",\n",
    "    timeout={\"attemptDurationSeconds\": timeout},\n",
    "    containerProperties={\n",
    "        \"image\": image,\n",
    "        \"executionRoleArn\": execution_role[\"Role\"][\"Arn\"],\n",
    "        \"jobRoleArn\": execution_role[\"Role\"][\"Arn\"],\n",
    "        \"resourceRequirements\": [\n",
    "            {\"value\": str(vcpu), \"type\": \"VCPU\"},\n",
    "            {\"value\": str(memory), \"type\": \"MEMORY\"},\n",
    "        ],\n",
    "        \"networkConfiguration\": {\"assignPublicIp\": \"ENABLED\"},\n",
    "        \"secrets\": [\n",
    "            {\n",
    "                \"name\": \"DESCARTESLABS_CLIENT_ID\",\n",
    "                \"valueFrom\": f\"{dl_auth_secret_name}:client_id::\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"DESCARTESLABS_CLIENT_SECRET\",\n",
    "                \"valueFrom\": f\"{dl_auth_secret_name}:client_secret::\"\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "    platformCapabilities=[\n",
    "        \"FARGATE\",\n",
    "    ],\n",
    "    retryStrategy={\n",
    "        \"attempts\": retries,\n",
    "    },\n",
    ")\n",
    "job_definition_arn = response[\"jobDefinitionArn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e85ece-37d7-40bc-9001-af771723ebf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(job_queue_arn, job_definition_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c9c461-9d69-46fe-a2d3-8757f5d1d67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_definition_arn = \"arn:aws:batch:us-west-2:851517463584:job-definition/dl-aws-onboarding-batch-definition:9\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305fae75-262b-402c-ae61-1c17440c5afd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Submitting jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abc6218-d5b4-4cfd-a531-47c968a746de",
   "metadata": {},
   "source": [
    "With our job queue and definition created we can now start submitting jobs. To do this you can use the simple function below. The function takes a geometry, a field identifier (unique id), and the S3 bucket and model name for the model you had previously stored (please see the Lambda notebook for information on this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19f243d-5f31-4398-bf1d-47c2e7c2f2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_job(\n",
    "    geom, \n",
    "    fid, \n",
    "    s3_bucket,\n",
    "    model_name\n",
    "):\n",
    "    cmd = [\n",
    "        json.dumps(geom),\n",
    "        fid,\n",
    "        s3_bucket,\n",
    "        model_name\n",
    "    ]\n",
    "\n",
    "    response = batch.submit_job(\n",
    "        jobName=f\"dl_ea_onboarding_class_fid-{fid}\",\n",
    "        jobQueue=job_queue_arn,\n",
    "        jobDefinition=job_definition_arn,\n",
    "        containerOverrides={\"command\": cmd}\n",
    "    )\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb1cef8-ddfd-49f3-a106-d6521c2c796a",
   "metadata": {},
   "source": [
    "We can submit a job using a simple test geometry to start:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e4bfd6-1de3-463c-a45e-9447abd0f9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_geom = {\n",
    "    \"type\": \"Polygon\",\n",
    "    \"coordinates\": [\n",
    "      [\n",
    "        [\n",
    "          -91.55319213867188,\n",
    "          35.805249625952506\n",
    "        ],\n",
    "        [\n",
    "          -91.54885768890381,\n",
    "          35.805249625952506\n",
    "        ],\n",
    "        [\n",
    "          -91.54885768890381,\n",
    "          35.80895624882348\n",
    "        ],\n",
    "        [\n",
    "          -91.55319213867188,\n",
    "          35.80895624882348\n",
    "        ],\n",
    "        [\n",
    "          -91.55319213867188,\n",
    "          35.805249625952506\n",
    "        ]\n",
    "      ]\n",
    "    ]\n",
    "  }\n",
    "\n",
    "test_fid = \"test-field\"\n",
    "s3_bucket = \"dl-aws-onboarding\"\n",
    "model_name = \"classifier.joblib\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8792a45-9d63-44a1-b136-00c20ff9ed2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_response = submit_job(test_geom, test_fid, s3_bucket, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ff9a84-e712-4e18-aec1-ba5c73ad64f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba814fc6-07bb-4033-bbf1-99b51a62c486",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Submitting multiple jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c48292-be81-48cb-923b-d5f9a5afc81b",
   "metadata": {},
   "source": [
    "The final piece will be to submit a list of jobs to Batch to be run asynchronously. To do this we will load a few hundred agricultural fields in Iowa and submit the geometries to our queue. We load the geojson data into a GeoPanda DataFrame and visualize the fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffce0db-227c-4289-b433-a245885556da",
   "metadata": {},
   "outputs": [],
   "source": [
    "ia_fields = gpd.read_file(\"ia_test_fields.geojson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74ecfd2-6514-4950-8ef8-4a377bc32b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "ia_fields.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199062f1-af6a-4ae0-bfbb-7c1d7b268877",
   "metadata": {},
   "outputs": [],
   "source": [
    "ia_fields.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee817a1-d1f6-49ae-838d-894879ae0117",
   "metadata": {},
   "source": [
    "We specify the location of our model in S3 and then for each field in our DataFrame submit a job to our queue. We need to make sure we submit json formatted geometries so we use `shapely` to map the geometries as a json-like object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90379b7b-4677-4947-bb46-c6d3a17060d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket = \"dl-aws-onboarding\"\n",
    "model_name = \"classifier.joblib\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c505ab-037e-49fc-a4cc-729b90f399f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = []\n",
    "for index, row in ia_fields.iterrows():\n",
    "    # print(sg.mapping(row[\"geometry\"]), row[\"FBndID\"], s3_bucket, model_name)\n",
    "    jobs.append(submit_job(sg.mapping(row[\"geometry\"]), row[\"FBndID\"], s3_bucket, model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531bc0d2-2dca-4dcf-aa74-b4021705e61d",
   "metadata": {},
   "source": [
    "Now we can visit our Batch dashboard to watch as our tasks are run!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a2744d-49db-4eca-9ff5-e2a26f29dd90",
   "metadata": {},
   "source": [
    "<img src=\"../images/batch_dashboard.png\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b120eb-3559-483d-85ac-1090816b10c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3e90fd-6891-4d48-b167-3e70e9c72e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc336e6-4718-439f-8923-9916c722c152",
   "metadata": {},
   "source": [
    "Congrats! You have now deployed a model at scale leveraging the DL platform and AWS Batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880be6d4-437b-4ff7-b544-b6de108850f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
