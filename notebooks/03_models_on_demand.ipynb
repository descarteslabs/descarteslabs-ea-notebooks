{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c573960-38cb-41c0-ba8b-cfbe048e6cd9",
   "metadata": {},
   "source": [
    "# Running models on demand using DL & AWS Lambda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b092aa87-e863-4d44-a7a9-e949fdd03283",
   "metadata": {},
   "source": [
    "In this notebook we will be exploring what using DL in a production pipeline or application might look like. To do this we will deploy a pre-trained ML model on AWS Lambda that uses Sentinel-2 data accessed with the Descartes Labs platform. The hypothetical \"user\" of our Lambda function will submit a geometry and unique id in a request and receive the model output back."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06af0cd-2c33-4436-8896-1445d7979484",
   "metadata": {},
   "source": [
    "## Runtime code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7260c951-7dca-4b8b-bbb6-0af4bfca18b1",
   "metadata": {},
   "source": [
    "To start we need to build our \"application\". Included in `../app` you will find several files. The relevant ones for this notebook are: `dl_aws_lambda.py`, `model.py`, and `utils.py`. These files have code for accessing imagery from the DL platform, pulling a model from an AWS S3 bucket, applying that model, then returning the model output. Let's take a closer looks at each of these parts:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9b5480-635f-4127-8af9-6daf41982f19",
   "metadata": {},
   "source": [
    "The main function in `model.py` is `get_field_class`. This function accesses the model, pulls the imagery, and returns a model output. The inputs for this function are a valid json geometry, a unique field identifier, the S3 bucket where the model lives, and the name of them model. This function can be seen below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df1b4b7-a617-4ff0-b5f9-4e81589306e7",
   "metadata": {},
   "source": [
    "```python\n",
    "def get_field_class(geom, fid, s3_bucket, model_name):\n",
    "    \n",
    "    if not osp.exists(\"/tmp/models\"):\n",
    "        os.makedirs(\"/tmp/models\")\n",
    "\n",
    "    temp_file_path = osp.join(\"/tmp/models\", model_name)\n",
    "\n",
    "    logger.info(f\"Loading classifier: {model_name}\")\n",
    "    if not osp.exists(temp_file_path):\n",
    "        s3 = boto3.client(\"s3\")\n",
    "        logger.info(f\"Classifier not found locally at {temp_file_path}. Pulling from s3\")\n",
    "        # Download pickled model from S3 and unpickle\n",
    "        s3.download_file(s3_bucket, model_name, temp_file_path)\n",
    "        \n",
    "    clf = load(temp_file_path)\n",
    "\n",
    "    logger.info(\"Retrieving timeseries\")\n",
    "    ndvi_ts, ndvi_dates = get_ndvi_tseries(geom)\n",
    "\n",
    "    \n",
    "    result = {\n",
    "        \"class\": clf.predict(ndvi_ts.reshape(1,-1))[0], \n",
    "        \"fid\": fid\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14705d1-31a0-4763-8c61-d2cf1ffd5cc6",
   "metadata": {},
   "source": [
    "This function first pulls a model (in our case an sklearn crop type classifcation model) from an S3 bucket then accesses imagery over the provided geometry using the `get_ndvi_tseries` function found in `../app/utils.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de314b94-2637-4d7e-af75-ed38e70a1c24",
   "metadata": {},
   "source": [
    "```python\n",
    "def get_ndvi_tseries(\n",
    "    geom, \n",
    "    start_date=\"2019-04-01\", \n",
    "    end_date=\"2019-10-01\"\n",
    "):\n",
    "\n",
    "    logging.info(\"Searching for S2 L2A scenes\")\n",
    "    scenes, ctx = dl.scenes.search(\n",
    "        geom,\n",
    "        products=\"esa:sentinel-2:l2a:v1\",\n",
    "        start_datetime=start_date,\n",
    "        end_datetime=end_date,\n",
    "        limit=None,\n",
    "    )\n",
    "    logging.info(f\"Found {len(scenes)} scenes for specified geometry\")\n",
    "    \n",
    "    logging.info(f\"Pulling raster data from DL Catalog\")\n",
    "    stack = scenes.stack(\n",
    "        [\"red\", \"nir\", \"cloud_mask\"],\n",
    "        ctx,\n",
    "        flatten=lambda x: x.properties.date.strftime(\"%Y-%m-%d\"),\n",
    "        scaling=\"physical\",\n",
    "        progress=False,\n",
    "    )\n",
    "    \n",
    "    logging.info(f\"Masking out clouds\")\n",
    "    cmask = np.repeat(\n",
    "        (stack[:,-1].data==1)[:, np.newaxis],\n",
    "        stack.shape[1],\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    stack.mask = (stack.mask) | cmask\n",
    "    \n",
    "    logging.info(f\"Computing NDVI\")\n",
    "    ndvi = (stack[:,1] - stack[:,0])/(stack[:,1] + stack[:,0])\n",
    "\n",
    "    ndvi_ts = np.ma.median(ndvi, axis=[1,2])\n",
    "    dates = list(scenes.groupby(\"properties.date.day\"))\n",
    "    \n",
    "    dates = [\n",
    "        key for key, scene in scenes.groupby(\n",
    "            lambda x: x.properties.date.strftime(\"%Y-%m-%d\")\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    dates_ts = [\n",
    "        datetime.strptime(date, \"%Y-%m-%d\").timestamp() for date in dates\n",
    "    ]\n",
    "    \n",
    "    new_dates = np.arange(\n",
    "        datetime.strptime(start_date, \"%Y-%m-%d\"),\n",
    "        datetime.strptime(end_date, \"%Y-%m-%d\"),\n",
    "        timedelta(days=6)\n",
    "    ).astype(datetime)\n",
    "    \n",
    "    new_dates_ts = [t.timestamp() for t in new_dates]\n",
    "    \n",
    "    tseries_masked = ndvi_ts.data[~ndvi_ts.mask]\n",
    "    dates_masked = np.array(dates_ts)[~ndvi_ts.mask]\n",
    "    \n",
    "    logging.info(f\"Interpolating time series from dates: {dates} to new dates: {new_dates.tolist()}\")\n",
    "    \n",
    "    f_interp = interp1d(\n",
    "        dates_masked,\n",
    "        tseries_masked,\n",
    "        bounds_error=False,\n",
    "        copy=False,\n",
    "        fill_value=\"extrapolate\",\n",
    "    )\n",
    "    \n",
    "    return f_interp(new_dates_ts)[1:], new_dates[1:]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d126537-7fde-4f23-bb1c-1a4d7f4cac40",
   "metadata": {},
   "source": [
    "This function searches for Sentinel-2 L2A imagery over the geometry, pulls that imagery, applies a cloud mask, computes a vegetative index (NDVI), interpolates the imagery onto regular time intervals, and then returns the intepolated array. This array will have shape (n timesteps, n features, pixel x, pixel y). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1faa7be3-ec1a-4f7d-9427-ec44a7d900b3",
   "metadata": {},
   "source": [
    "The code in `dl_aws_lambda.py` simply takes an event of the type we would expect to receive in Lambda, parses that event to get a geometry and field id, then returns the output of the `get_field_class` function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b82daf6-903c-4e8f-8bc3-e23c5e882e86",
   "metadata": {},
   "source": [
    "We'll need to make sure that we have a model to run! To do this we will put the model found in `../models/classifier.joblib` into an S3 bucket. For more info on using S3 please consult the [AWS docs here](https://docs.aws.amazon.com/AmazonS3/latest/userguide/setting-up-s3.html). We can point to this bucket by setting envinronment variables in our Docker image or by specifying them in the configuration of our Lambda function.\n",
    "\n",
    "<img src=\"../images/s3_model_bucket.png\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae2e767-6970-47a7-a538-ff2f51ecaa1d",
   "metadata": {},
   "source": [
    "## Building a Docker image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f62f7bf-02bf-4a35-a28c-6d285c7e0c6c",
   "metadata": {},
   "source": [
    "Now that we have code to run we need to build a Docker Image for Lambda that has the Descartes Labs client installed. We have provided a Dockerfile that will do just that. The Dockerfile can be broken down into a few parts:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0d31b4-891f-4df0-93db-4f4c40178b61",
   "metadata": {},
   "source": [
    "```dockerfile\n",
    "FROM public.ecr.aws/lambda/python:3.9\n",
    "\n",
    "COPY app ${LAMBDA_TASK_ROOT}/app\n",
    "\n",
    "COPY requirements.txt requirements.txt\n",
    "RUN pip3 install -r requirements.txt --target \"${LAMBDA_TASK_ROOT}\"\n",
    "RUN pip3 install -U descarteslabs>=1.11.0 --target \"${LAMBDA_TASK_ROOT}\"\n",
    "\n",
    "ENV DESCARTESLABS_ENV=aws-production\n",
    "\n",
    "RUN mkdir /tmp/models\n",
    "\n",
    "# ENV MODEL_NAME=classifier.joblib\n",
    "# ENV MODEL_S3_BUCKET=dl-aws-onboarding\n",
    "\n",
    "CMD [ \"app.dl_aws_lambda.run_model\" ]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c30156d-0533-448b-bf2e-532d349bd079",
   "metadata": {},
   "source": [
    "We use a base Lambda Python3 image and copy our \"application\" code into the image. We then install the Descartes Labs client with version >= 1.11.0. Next we install a set of Python package requirements found in `requirements.txt`. We set the `DESCARTESLAB_ENV` variable to use the DL AWS services. Finally we create a temp directory for our model and then specify what code we want executed at runtime (specified by `CMD`.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5232868-9b1f-42f1-b581-4973331c2306",
   "metadata": {},
   "source": [
    "We now need to build the Docker image and push to the AWS Elastic Container Registry to be used in Lambda. For more infomation on how to do this please see the following AWS documentation:\n",
    "- [Creating an ECR repository](https://docs.aws.amazon.com/AmazonECR/latest/userguide/repository-create.html)\n",
    "- [Build and push your Docker image](https://docs.aws.amazon.com/lambda/latest/dg/images-create.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7036b026-fb4f-4fc6-8135-0b7ed5520f99",
   "metadata": {},
   "source": [
    "A brief summary of these steps can be found below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e95e0bd-ddd5-4252-b3ab-1378bfdb5d8c",
   "metadata": {},
   "source": [
    "1) `cd ~/dl-ea-aws-onboarding`\n",
    "2) `docker build -t dl-aws-onboarding-lambda -f dockerfiles/lambda/Dockerfile .` You can specify a different name for your image by swapping our \"dl-aws-onboarding-lambda\" for something else\n",
    "3) `docker tag dl-aws-onboarding-lambda:latest {your-container-registry}/dl-aws-onboarding-lambda:latest`\n",
    "4) `docker push {your-container-registry}/dl-aws-onboarding-lambda:latest`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd59add5-140b-4162-aec5-a17c28f5f419",
   "metadata": {},
   "source": [
    "With our Docker image now in the ECR we can create our Lambda function. For more information on the basics of creating a function on Lambda please see the [AWS docs here](https://docs.aws.amazon.com/lambda/latest/dg/getting-started.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa5bb93-de06-427f-ad03-fdf7a848c59d",
   "metadata": {},
   "source": [
    "## Creating our Lambda function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f22ed3-ecc2-42ca-9f9b-0f556165b345",
   "metadata": {},
   "source": [
    "We create a new function and select \"Container function\".\n",
    "\n",
    "<img src=\"../images/create_function_container.png\" align=\"center\"/>\n",
    "\n",
    "Then we can give our function a name and provide the URI pointing to the image we created and pushed to the ECR.\n",
    "\n",
    "<img src=\"../images/ecr_uri.png\" align=\"center\"/>\n",
    "\n",
    "You may need to adjust the execution role to [provide access to the S3 bucket you created earlier](https://aws.amazon.com/premiumsupport/knowledge-center/lambda-execution-role-s3-bucket/). \n",
    "\n",
    "Finally create the function!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12f3ef3-6488-4d56-aae7-b93a1e4f6974",
   "metadata": {},
   "source": [
    "The last step now before we can test our Lambda function is to set some environment variables in the function configuration.\n",
    "\n",
    "We need to set the following variables:\n",
    "\n",
    "- MODEL_NAME\n",
    "- MODEL_S3_BUCKET\n",
    "- DESCARTESLABS_CLIENT_ID\n",
    "- DESCARTESLABS_CLIENT_SECRET\n",
    "\n",
    "Your client ID and secret can be found after authenticating with your DL account at `~/.descarteslabs/token_info.json`. These are required to access data using the DL platform.\n",
    "\n",
    "<img src=\"../images/env_vars_lambda.png\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddea276b-cf7a-4e34-a0cd-5857f328a73e",
   "metadata": {},
   "source": [
    "## Testing the Lambda function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c42a95-be7e-4f46-bf0d-1179c4af515b",
   "metadata": {},
   "source": [
    "We can now test our Lambda function! To do this we can navigate to \"Test\" and construct an example request. We must provide a valid json geometry and a unique identifier. You can use the request below as a reference:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"geometry\": {\n",
    "    \"type\": \"Polygon\",\n",
    "    \"coordinates\": [\n",
    "      [\n",
    "        [\n",
    "          -91.55319213867188,\n",
    "          35.805249625952506\n",
    "        ],\n",
    "        [\n",
    "          -91.54885768890381,\n",
    "          35.805249625952506\n",
    "        ],\n",
    "        [\n",
    "          -91.54885768890381,\n",
    "          35.80895624882348\n",
    "        ],\n",
    "        [\n",
    "          -91.55319213867188,\n",
    "          35.80895624882348\n",
    "        ],\n",
    "        [\n",
    "          -91.55319213867188,\n",
    "          35.805249625952506\n",
    "        ]\n",
    "      ]\n",
    "    ]\n",
    "  },\n",
    "  \"fid\": \"test-field\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ef966f-cdb0-4ee1-852b-a72debd5e018",
   "metadata": {},
   "source": [
    "We can then click \"Test\" and have the Lambda function process the request. You should see a response json that looks something like this:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"class\": 0,\n",
    "    \"fid\": \"test-field\"\n",
    "}\n",
    "```\n",
    "Class will be either 0 for not-corn or 1 for corn.\n",
    "\n",
    "You may also need to adjust the available memory for your function under `General configuration` depending on the size of your test request."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff734b92-674a-4b94-b57a-a53edf2f7ebd",
   "metadata": {},
   "source": [
    "From here you can now explore a variety of ways to trigger your Lambda function. [You can add triggers](https://docs.aws.amazon.com/lambda/latest/dg/lambda-invocation.html) and [create destinations](https://aws.amazon.com/blogs/compute/introducing-aws-lambda-destinations/) for the output of your Lambda function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
